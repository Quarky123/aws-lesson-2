{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP II: `CountVectorizer` and `TfidfVectorizer`\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Justin Pounders (ATL), Riley Dallas (AUS)_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](https://snag.gy/uvESGH.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Extract features from unstructured text with `sklearn`\n",
    "- Describe how count vectorization and TF-IDF work.\n",
    "- Implement `CountVectorizer` in a spam classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "---\n",
    "\n",
    "We'll need the following libraries for today's lecture:\n",
    "- `pandas`\n",
    "- `CountVectorizer` and `TfidfVectorizer` from `sklearn.feature_extraction.text`\n",
    "- `Pipeline`\n",
    "- `train_test_split` and `GridSearchCV`\n",
    "- `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to Text Feature Extraction\n",
    "\n",
    "---\n",
    "\n",
    "The models we’ve been using so far accept a two-dimensional matrix of real numbers as input `X` and a target vector of classes or numbers as `y`. What if our features are not given in the form of a table of numbers but rather are unstructured? This is the case when we work with text documents.\n",
    "\n",
    "> We need a way to go from unstructured data to our numeric `X` matrix in order to use the same models. This is called _feature extraction_, and this lesson is dedicated to it.\n",
    "\n",
    "The applications of using text data in statistical modeling are practically infinite. Some examples include:\n",
    "\n",
    "- Sentiment analysis of Yelp reviews.\n",
    "- Identifying topics of new articles.\n",
    "- Classification of political authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='simple'></a>\n",
    "## A Simple Example\n",
    "---\n",
    "\n",
    "Suppose we're building a model that predicts whether a sentence is from a children's book or not. The inputs are strings and the output is a binary label.\n",
    "\n",
    "Here are some sample inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'Spot is a dog.'\n",
    "s2 = 'Run Spot Run.'\n",
    "s3 = 'Run Forrest, Run!'\n",
    "s4 = 'The quick brown fox jumped over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "Virtually all NLP uses this base terminology:\n",
    "\n",
    "- a collection of text is a **document**. You can think of a document as a row in your feature matrix.\n",
    "- a collection of documents is a **corpus** (plural corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus\n",
    "train_sentences =\n",
    "test_sentences ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bow'></a>\n",
    "## Bag of Words/Word Counting\n",
    "---\n",
    "\n",
    "The bag-of-words model is a simplified representation of the raw data. In this model, text (such as a sentence or document) is represented as the bag (multiset) of its words.\n",
    "\n",
    "Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty dictionary.\n",
    "bag_of_words = {}\n",
    "\n",
    "# Iterate through each word in each sentence.\n",
    "for sentence in [s1, s2, s3, s4]:\n",
    "    for word in sentence.split():\n",
    "        \n",
    "        # Create a key-value pair in the dictionary for each word,\n",
    "        # with the key being the word and the value = 0.\n",
    "        # And assign a value of 0. \n",
    "        bag_of_words[word] = 0\n",
    "        \n",
    "# Iterate back through each word in each sentence.\n",
    "for sentence in [s1, s2, s3, s4]:\n",
    "    for word in sentence.split():\n",
    "        \n",
    "        # Every time we see word again, add 1 to the value.\n",
    "        bag_of_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the advantages of the bag-of-words approach?</summary>\n",
    "\n",
    "- Efficient to store.\n",
    "- Efficient to model.\n",
    "- Keeps a decent amount of information.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the disadvantages of the bag-of-words approach?</summary>\n",
    "\n",
    "- Since bag-of-words models discard grammar, order, structure, and context, we lose a decent amount of information.\n",
    "- Phrases like \"not bad\" or \"not good\" won't be interpreted properly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"countvectorizer\"></a>\n",
    "## Demo: Scikit-Learn `CountVectorizer`\n",
    "---\n",
    "\n",
    "Scikit-learn offers a `CountVectorizer` class with many configurable options:\n",
    "\n",
    "**Note**: There are several parameters to tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer\n",
    "cvec ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer on our corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus\n",
    "X_train = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train into a DataFrame\n",
    "X_train_df ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test\n",
    "X_test =\n",
    "X_test_df ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "## Stop Words\n",
    "\n",
    "---\n",
    "\n",
    "Some words are commonly used and provide no legitimate information about the content of the text. This isn't always going to be the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` gives you the option to eliminate stop words from your corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stop words that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be an example of a situation in which we do not want to remove stopwords?</summary>\n",
    "    \n",
    "- Any case where stopwords are important/predictive, we'd want to keep them.\n",
    "    - Poetry.\n",
    "    - Chatbots.\n",
    "    - Translation services. (e.g. Google Translate)\n",
    "- If our data set is a small size, there may not be a reason to remove stopwords.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get rather large. To mitigate this problem, you can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Range\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams (aka bi-grams) are:\n",
    "- 'the quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumped'\n",
    "- 'jumped over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog'\n",
    "\n",
    "And the 3-grams are:\n",
    "- 'the quick brown'\n",
    "- 'quick brown fox'\n",
    "- 'brown fox jumped'\n",
    "- 'fox jumped over'\n",
    "- 'jumped over the'\n",
    "- 'over the lazy'\n",
    "- 'the lazy dog'\n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(1,2)) # Captures every single word and every 2-gram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min/Max Document Frequency\n",
    "---\n",
    "\n",
    "We can tell `CountVectorizer` to only consider words that occur within a certain threshold in the corpus.\n",
    "\n",
    "For example, if we only want `CountVectorizer` to add words that occur in **at least** two documents, we can do the following:\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "Conversely, we can set an upper threshold with `max_df`:\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classification model\n",
    "---\n",
    "Let's test this stuff out on some SMS text data.  Can you predict real vs. promotional texts just based on what is written?  Let's see...\n",
    "\n",
    "> This data set was taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "---\n",
    "\n",
    "Convert ham/spam into binary labels:\n",
    "- 0 for ham\n",
    "- 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy\n",
    "---\n",
    "\n",
    "We need to calculate baseline accuracy in order to tell if our model is outperforming the null model (predicting the majority class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep\n",
    "---\n",
    "\n",
    "Let's set up our data for modeling:\n",
    "- `X` will be the `message` column. **NOTE**: `CountVectorizer` requires a vector, so make sure you set `X` to be a `pandas` Series, **not** a DataFrame.\n",
    "- `y` will be the `label` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =\n",
    "y ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "---\n",
    "\n",
    "Use the train_test_split function to split your data into a training set and a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "---\n",
    "\n",
    "Our pipeline will consist of two stages:\n",
    "1. An instance of `CountVectorizer`\n",
    "2. A `LogisticRegression` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV`\n",
    "---\n",
    "\n",
    "At this point, you could use your `pipeline` object as a model:\n",
    "\n",
    "```python\n",
    "# Evaluate how your model will perform on unseen data\n",
    "cross_val_score(pipe, X_train, y_train, cv=3).mean() \n",
    "\n",
    "# Fit your model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "pipe.score(X_train, y_train)\n",
    "\n",
    "# Test score\n",
    "pipe.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "Since we want to tune over the `CountVectorizer`, so we'll load our `pipeline` object into `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"tfidf\"></a>\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "---\n",
    "\n",
    "A TF-IDF score tells us which words are most discriminating between documents. Words that occur often in one document but don't occur in many documents contain a great deal of discriminating power.\n",
    "\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection (corpus).\n",
    "- The importance increases in proportion to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Variations of the TF-IDF weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides — that is, whether the term is common or rare across all documents. It's the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term and then taking the logarithm of that quotient.\n",
    "\n",
    "**Let's see how it's calculated:**\n",
    "\n",
    "Term frequency (`tf`) is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{term}$ is the number of times a term/word $t$ appears in document $d$\n",
    "- $N_\\text{terms in Document}$ is the number of terms/words in document $d$\n",
    "\n",
    "Inverse document frequency (`idf`) is defined as the frequency of documents that contain that term over the whole corpus:\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $N_\\text{Documents}$ is the number of documents in the corpus $D$\n",
    "- $N_\\text{Documents that contain term}$ is the number of documents in $D$ that contain term/word $t$\n",
    "\n",
    "TF-IDF is then calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **You might ask: But what is `log` used for?**<br>\n",
    "> Good question! This is a sublinear transformation that helps separate our extremes between rare and common values.\n",
    "\n",
    "> \"...any linear function, ${\\displaystyle g}$, for sufficiently large input ${\\displaystyle f}$, grows slower than ${\\displaystyle g}$\" — Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='tfidf-vec'></a>\n",
    "## Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use TF-IDF?\n",
    "- Common words are penalized.\n",
    "- Rare words have more influence.\n",
    "\n",
    "Scikit-learn provides a TF-IDF vectorizer that works similarly to the other vectorizers we've covered. Notice that we can also eliminate stop words to improve our analysis.\n",
    "\n",
    "As you did above, import and initialize the `TfidfVectorizer`, then fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"am a cat\", \"am a rat\", \"am a bat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the transformer\n",
    "tvec = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df  ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- Check out this [Yelp blog post](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) on how it completed a classification task (with more than 1,000 response variables) using restaurant review text.\n",
    "- Always check documentation: \n",
    "    - [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "    - [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). \n",
    "    - [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "- A list of all stop words is available [here](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-05/5.04-nlp/stop-words.txt).\n",
    "- Wikipedia's [feature hashing](https://github.com/generalassembly-studio/DSI-course-materials/tree/master/curriculum/04-lessons/week-06/4.1-lesson) and [hash functions](https://en.wikipedia.org/wiki/Hash_function) entries are a great place to turn for more information on hashing.\n",
    "- Check out Charlie Greenbacker's [introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf), which he delivered at the [DC-NLP Meetup](http://www.meetup.com/DC-NLP/).\n",
    "- Wikipedia also has a [walk through](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) of TF-IDF.\n",
    "- We played with Google's [ngram tool](https://books.google.com/ngrams/graph?content=data+science&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cdata%20science%3B%2Cc0).\n",
    "- A hilarious data scientist has gone rogue and used NLP and eigenfaces (eigenvalues for face recognition) [for Tinder](http://dataconomy.com/hacking-tinder-with-facial-recognition-nlp/).\n",
    "- We referenced KPCB's 2016 internet trends. If you're into startups, check out [this insightful deck](http://www.kpcb.com/internet-trends).\n",
    "- [Count vectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "- [Choosing a stemmer](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html).\n",
    "- [Feature hashing](https://en.wikipedia.org/wiki/Feature_hashing).\n",
    "- [Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "- [TF-IDF vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
